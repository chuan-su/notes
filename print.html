<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title></title>
        
        <meta name="robots" content="noindex" />
        
        


        <!-- Custom HTML head -->
        


        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        
        <link rel="icon" href="favicon.svg">
        
        
        <link rel="shortcut icon" href="favicon.png">
        
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        
        <link rel="stylesheet" href="css/print.css" media="print">
        

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        
        <link rel="stylesheet" href="fonts/fonts.css">
        

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        

        
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="index.html">Chuan's Notes</a></li><li class="chapter-item expanded "><a href="java/index.html"><strong aria-hidden="true">1.</strong> Java</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="java/reduce.html"><strong aria-hidden="true">1.1.</strong> reduce</a></li><li class="chapter-item expanded "><a href="java/foldleft.html"><strong aria-hidden="true">1.2.</strong> foldleft</a></li></ol></li><li class="chapter-item expanded "><a href="docker/index.html"><strong aria-hidden="true">2.</strong> Docker</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="docker/dockerfile.html"><strong aria-hidden="true">2.1.</strong> Dockerfile</a></li><li class="chapter-item expanded "><a href="docker/volume.html"><strong aria-hidden="true">2.2.</strong> volume</a></li><li class="chapter-item expanded "><a href="docker/network.html"><strong aria-hidden="true">2.3.</strong> network</a></li><li class="chapter-item expanded "><a href="docker/dockerrun.html"><strong aria-hidden="true">2.4.</strong> docker run</a></li></ol></li><li class="chapter-item expanded "><a href="aws/index.html"><strong aria-hidden="true">3.</strong> AWS</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="aws/ebs.html"><strong aria-hidden="true">3.1.</strong> EBS</a></li><li class="chapter-item expanded "><a href="aws/ec2.html"><strong aria-hidden="true">3.2.</strong> EC2</a></li><li class="chapter-item expanded "><a href="aws/iam.html"><strong aria-hidden="true">3.3.</strong> IAM</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="aws/iamrole.html"><strong aria-hidden="true">3.3.1.</strong> IAM Role</a></li></ol></li><li class="chapter-item expanded "><a href="aws/vpc.html"><strong aria-hidden="true">3.4.</strong> VPC</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="aws/hybrid-cloud.html"><strong aria-hidden="true">3.4.1.</strong> Hybrid Cloud</a></li></ol></li><li class="chapter-item expanded "><a href="aws/elb.html"><strong aria-hidden="true">3.5.</strong> ELB</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="aws/auto-scaling.html"><strong aria-hidden="true">3.5.1.</strong> Auto Scaling</a></li></ol></li><li class="chapter-item expanded "><a href="aws/route53.html"><strong aria-hidden="true">3.6.</strong> Route 53</a></li><li class="chapter-item expanded "><a href="aws/global-accelerator.html"><strong aria-hidden="true">3.7.</strong> Global Accelerator</a></li><li class="chapter-item expanded "><a href="aws/cloudfront.html"><strong aria-hidden="true">3.8.</strong> CloudFront</a></li><li class="chapter-item expanded "><a href="aws/storage.html"><strong aria-hidden="true">3.9.</strong> Storage</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="aws/s3.html"><strong aria-hidden="true">3.9.1.</strong> S3 and Glacier</a></li><li class="chapter-item expanded "><a href="aws/replication.html"><strong aria-hidden="true">3.9.2.</strong> Replication</a></li></ol></li><li class="chapter-item expanded "><a href="aws/elasticache.html"><strong aria-hidden="true">3.10.</strong> ElastiCache</a></li><li class="chapter-item expanded "><a href="aws/security.html"><strong aria-hidden="true">3.11.</strong> Security</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="aws/data-encryption.html"><strong aria-hidden="true">3.11.1.</strong> Data Encryption</a></li></ol></li></ol></li><li class="chapter-item expanded "><a href="linux/index.html"><strong aria-hidden="true">4.</strong> Linux</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="linux/volume.html"><strong aria-hidden="true">4.1.</strong> Volume</a></li><li class="chapter-item expanded "><a href="linux/void.html"><strong aria-hidden="true">4.2.</strong> Void Linux</a></li><li class="chapter-item expanded "><a href="linux/netcat.html"><strong aria-hidden="true">4.3.</strong> netcat</a></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                        
                    </div>

                    <h1 class="menu-title"></h1>

                    <div class="right-buttons">
                        
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        
                        
                        

                    </div>
                </div>

                
                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="chuans-notes"><a class="header" href="#chuans-notes">Chuan's Notes</a></h1>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><h1 id="java"><a class="header" href="#java">Java</a></h1>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><h1 id="reduce-in-java"><a class="header" href="#reduce-in-java"><code>reduce</code> in Java</a></h1>
<p>The <a href="https://docs.oracle.com/javase/8/docs/api/java/util/stream/Stream.html#reduce-T-java.util.function.BinaryOperator-">reduce</a> function below will just work fine.</p>
<pre><code class="language-java">Integer sum = integers.reduce(0, (a, b) -&gt; a+b);

</code></pre>
<p>However, when the arguments paassed to the <code>BinaryOperator&lt;T&gt;</code> accumulator are of different types, the compiler complains.</p>
<h2 id="example"><a class="header" href="#example">Example</a></h2>
<p>Given a list of bank transactions.</p>
<pre><code class="language-java">public class Transaction {
  double amount;
}
</code></pre>
<p>We want to calcuate the sum</p>
<pre><code class="language-java">List&lt;Transaction&gt; transactions; //...

double sum = transactions.stream()
    .reduce(0.0, (result, t) -&gt; result + t.getAmount(), Double::sum);

</code></pre>
<p>We have to provide a <code>Double::sum</code> combiner parameter for it to compile. Taking a closer look at the API:</p>
<pre><code class="language-java">&lt;U&gt; U reduce(U identity,
             BiFunction&lt;U,? super T,U&gt; accumulator,
             BinaryOperator&lt;U&gt; combiner)

</code></pre>
<p>The arguments of our accumulator above has different types which are of <code>Transaction</code> and <code>Double</code>. Therefore, we need the third combiner.</p>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><h1 id="foldleft-in-java"><a class="header" href="#foldleft-in-java"><code>foldLeft</code> in Java</a></h1>
<p>In Java, you can use the <a href="https://docs.oracle.com/javase/8/docs/api/java/util/stream/Collector.html">Collector</a> interface to perform <em>reduction</em> operations on a collection, which is similar to the <code>foldLeft</code> function in Scala.</p>
<pre><code class="language-java">static &lt;T,R&gt; Collector&lt;T,R,R&gt; of(Supplier&lt;R&gt; supplier,
                                 BiConsumer&lt;R,T&gt; accumulator,
                                 BinaryOperator&lt;R&gt; combiner,
                                 Collector.Characteristics... characteristics)
</code></pre>
<h2 id="example-1"><a class="header" href="#example-1">Example</a></h2>
<p>Given an example that one customer has multiple bank accounts:</p>
<pre><code class="language-java">public class BankAccount {
    String customerName;
    double balance;

    // getter, setters..
}

</code></pre>
<p>We want to group a list of bank accounts by customer name and find out the balance across the customers' bank accoutns.</p>
<pre><code class="language-java">List&lt;BankAccount&gt; bankAccounts; //...

Map&lt;String, Double&gt; customers = bankAccounts.stream()
    .collect(Collector.of(
        (Supplier&lt;HashMap&lt;String, Double&gt;&gt;) HashMap::new,
        (result, bankAccount) -&gt; {
            var sum = result.getOrDefault(bankAccount.getCustomerName(), 0.0);
            result.put(b.getCustomerName(), sum + bankAccount.getBalance());
        },
        (a, b) -&gt; a
    ));

</code></pre>
<p>This can also be addressed by using Java built-in <a href="https://docs.oracle.com/javase/8/docs/api/java/util/stream/Collectors.html#groupingBy-java.util.function.Function-java.util.stream.Collector-">groupingBy</a> collector:</p>
<pre><code class="language-java">bankAccounts.stream()
    .collect(Collectors.groupingBy(BankAccount::getCustomerName,
        Collectors.summingDouble(BankAccount::getBalance)));

</code></pre>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><h1 id="docker"><a class="header" href="#docker">Docker</a></h1>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><h1 id="dockerfile"><a class="header" href="#dockerfile">Dockerfile</a></h1>
<p>The default working directory is root <code>/</code>.</p>
<p><code>WORKDIR</code> is used to set working directory for any <code>RUN</code>, <code>COPY</code>, <code>ADD</code>, <code>ENTRYPOINT</code>, <code>CMD</code> instructions. And the directory is created if it does not exist.</p>
<h2 id="entrypoint-and-cmd"><a class="header" href="#entrypoint-and-cmd">ENTRYPOINT and CMD</a></h2>
<p><code>ENTRYPOINT</code> + <code>CMD</code> = default container command arguments</p>
<p>Thus</p>
<pre><code class="language-Dockerfile">ENTRYPOINT [&quot;/docker-entrypoint.sh&quot;]
CMD [&quot;java&quot;, &quot;-jar&quot;, &quot;app.jar&quot;]
</code></pre>
<p>Is equivalent to</p>
<pre><code class="language-Dockerfile">ENTRYPOINT [&quot;/docker-entrypoint.sh&quot;, &quot;java&quot;, &quot;-jar&quot;, &quot;app.jar&quot;]
</code></pre>
<h3 id="override-cmd-and-entrypoint"><a class="header" href="#override-cmd-and-entrypoint">Override CMD and ENTRYPOINT</a></h3>
<p>Specifying <code>CMD</code> in <code>Dockerfile</code> merely creates a default value and can be overriden by <code>docker run</code></p>
<p>For the <code>Dockerfile</code> above, if we we invoke</p>
<pre><code class="language-bash">docker run myservice java -DlogLevel=debug -jar app.jar
</code></pre>
<p>The container will be created with the following arguments:</p>
<pre><code class="language-Dockerfile">[&quot;/docker-entrypoint.sh&quot;, &quot;java&quot;, &quot;-DlogLevel=debug&quot; &quot;-jar&quot;, &quot;app.jar&quot;]
</code></pre>
<p>To override the <code>ENTRYPOINT</code> declared in a <code>Dockerfile</code>, specify <code>docker run --entrypoint</code> flag.</p>
<pre><code class="language-Dockefile">docker run --entrypoint /docker-entrypoint2.sh myservice
</code></pre>
<p>To reset the container entrypoint, pass an empty string:</p>
<pre><code class="language-Dockerfile">docker run --entrypoint=&quot;&quot; myservice bash
</code></pre>
<p><em>Note</em> this also overrides the <code>CMD</code> command with <code>bash</code>.</p>
<h2 id="arg"><a class="header" href="#arg">ARG</a></h2>
<pre><code class="language-Dockefile">ARG GCLOUD_SDK_VERSION=286.0.0-alpine

FROM google/cloud-sdk:$GCLOUD_SDK_VERSION
</code></pre>
<p>The <code>ARG</code> defines a variable that users can pass at image build-time</p>
<pre><code class="language-bash">docker build --build-arg GCLOUD_SDK_VERSION=290.0.0 .
</code></pre>
<p>Note the <code>.</code> dot is representing the context where the docker image is built. Typically for the <code>COPY context/path/file /container/workdir</code></p>
<p>To build the image from another <code>Dockerfile</code>:</p>
<pre><code class="language-bash">docker build --build-arg GCLOUD_SDK_VERSION=290.0.0 -f path/to/Dockefile .
</code></pre>
<p><em>Note</em>: The <code>ARG</code> declared before a <code>FROM</code> is outside of a build stage. So it can't be used in any instruction after a <code>FROM</code>. To use the default value of an <code>ARG</code>, re-redeclare it without a value:</p>
<pre><code class="language-Dockefile">ARG GCLOUD_SDK_VERSION=286.0.0-alpine

FROM google/cloud-sdk:$GCLOUD_SDK_VERSION

ARG GCLOUD_SDK_VERSION
RUN echo $GCLOUD_SDK_VERSION &gt; image_version
</code></pre>
<p>Reference: <a href="https://docs.docker.com/engine/reference/builder/#understand-how-arg-and-from-interact">https://docs.docker.com/engine/reference/builder/#understand-how-arg-and-from-interact</a></p>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><h1 id="volume"><a class="header" href="#volume">Volume</a></h1>
<pre><code class="language-Dockerfile">VOLUME [&quot;/data&quot;]
</code></pre>
<p>The <code>VOLUME</code> creates a mount point to <strong>the volume on the host</strong> that holds the data persisted by the docker container during container runtime.</p>
<p>When running a docker container, the volume (directory) is created at the Docker root directory of the host machine - <code>/var/lib/docker/volumes</code>.</p>
<p>The name of volume is autogenerated and extreamly long, which are often referred to as &quot;unnamed&quot; or &quot;anonymous&quot;.</p>
<p>However if the mount point is specified in the <code>docker run -v</code> or <code>docker run --mount</code> command, Docker will create and use the volume as specified on the host instead of the default volume specified in the Dockerfile.</p>
<h2 id="example-2"><a class="header" href="#example-2">Example</a></h2>
<p>In the offical <a href="https://github.com/docker-library/mysql/blob/master/8.0/Dockerfile.debian#L87">mysql Dockerfile</a>:</p>
<pre><code class="language-Dockerfile">VOLUME /var/lib/mysql
</code></pre>
<p>If we run the mysql container</p>
<pre><code class="language-bash">docker run mysql:8
</code></pre>
<p>The mysql container instance will use the default mount point which is specified by the <code>VOLUME</code> instruction in the Dockerfile. And in my host,the volume is created at</p>
<pre><code class="language-bash">/var/lib/docker/volumes/00b4488b017762870295a3894aa1d2ff2b3c6126e445273ef45e279f6ee8ddf9
</code></pre>
<p>If we run the mysql container</p>
<pre><code class="language-bash">docker run -v /my/own/datadir:/var/lib/mysql mysql:8
</code></pre>
<p>This command mounts <code>/my/own/datadir</code> directory on my host as <code>/var/lib/mysql</code> inside the container instead.</p>
<h2 id="where-to-store-data"><a class="header" href="#where-to-store-data">Where to Store Data</a></h2>
<p>There are several ways to store data used by applications that run in Docker containers. We encourage users of the mysql images to familiarize themselves with the options available, including:</p>
<ul>
<li>
<p>Let Docker manage the storage of your database data by <a href="https://docs.docker.com/engine/tutorials/dockervolumes/#adding-a-data-volume">writing the database files to disk on the host system using its own internal volume managemen</a>. This is the default and is easy and fairly transparent to the user. The downside is that the files may be hard to locate for tools and applications that run directly on the host system, i.e. outside containers.</p>
</li>
<li>
<p>Create a data directory on the host system (outside the container) and <a href="https://docs.docker.com/engine/tutorials/dockervolumes/#mount-a-host-directory-as-a-data-volume">mount this to a directory visible from inside the container</a>. This places the database files in a known location on the host system, and makes it easy for tools and applications on the host system to access the files. The downside is that the user needs to make sure that the directory exists, and that e.g. directory permissions and other security mechanisms on the host system are set up correctly.</p>
</li>
</ul>
<p>The Docker documentation is a good starting point for understanding the different storage options and variations, and there are multiple blogs and forum postings that discuss and give advice in this area. We will simply show the basic procedure here for the latter option above:</p>
<ol>
<li>
<p>Create a data directory on a suitable volume on your host system, e.g. /my/own/datadir.</p>
</li>
<li>
<p>Start your mysql container like this:</p>
</li>
</ol>
<pre><code class="language-bash">$ docker run --name some-mysql -v /my/own/datadir:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=my-secret-pw -d mysql:tag
</code></pre>
<p>The <code>-v /my/own/datadir:/var/lib/mysql</code> part of the command mounts the <code>/my/own/datadir</code> directory from the underlying host system as <code>/var/lib/mysql</code> inside the container, where MySQL by default will write its data files.</p>
<p>Reference:</p>
<ul>
<li><a href="https://hub.docker.com/_/mysql/">https://hub.docker.com/_/mysql/</a></li>
</ul>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><h1 id="network"><a class="header" href="#network">network</a></h1>
<h2 id="you-dont-need-docker-compose"><a class="header" href="#you-dont-need-docker-compose">You don't need <code>docker-compose</code></a></h2>
<p><code>docker-compose</code> by default creates a <code>bridge</code> docker network named <code>&lt;directory&gt;_default</code> for connecting docker containers. To specify different network name:</p>
<pre><code class="language-yaml">version: &quot;3.3&quot;
services:
  web:
    image: registry/image_name:version
    container_name: web
    networks:
      - skywalker 	

  postgres:
    image: registry/image_name:version
    container_name: postgres
    networks:
      - skywalker 	
  
networks:
  skywalker:
    driver: bridge  	
</code></pre>
<p>This will create a <code>bridge</code> docker network named <code>skywalker_docker</code>.</p>
<p>The reason why the containers are interconnected is that they share the same <code>docker bridged network</code>. The <code>docker-compose</code> yaml scripts can be easily rewritten using <code>docker network</code>.</p>
<pre><code class="language-bash">docker network create -d bridge skywalker

docker run -d --rm --network skywalker --name web registry/image_name:version
docker run -d --rm --network skywalker --name postgres registry/image_name:version
</code></pre>
<p>And the <code>web</code> container can communicate the <code>postgres</code> container through <code>postgres:5432</code>.</p>
<p><strong>Note:</strong> The <code>container_name</code> is the host inside the docker network.</p>
<p>To inspect the network:</p>
<pre><code class="language-bash">docker network inspect skywalker
</code></pre>
<p>Other useful commands can be found at <a href="https://docs.docker.com/engine/reference/commandline/network/">https://docs.docker.com/engine/reference/commandline/network/</a>.</p>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><h1 id="docker-run"><a class="header" href="#docker-run">docker run</a></h1>
<h2 id="docker-run-image-multiple-commands"><a class="header" href="#docker-run-image-multiple-commands">docker run <IMAGE> <MULTIPLE COMMANDS></a></h2>
<pre><code class="language-bash">docker run - w /usr/src/app image /bin/bash -c &quot;mvn clean package;jar -jar app.jar&quot;
</code></pre>
<p><a href="https://docs.docker.com/engine/reference/commandline/run/#set-working-directory--w">Set working directory</a></p>
<p>The <code>-w</code> lets the command being executed inside the given directory, here <code>/usr/src/app</code>. If the path does not exit it is created inside the container.</p>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><h1 id="aws"><a class="header" href="#aws">AWS</a></h1>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><h1 id="ebs"><a class="header" href="#ebs">EBS</a></h1>
<p><img src="aws/../img/aws-ebs.svg" alt="ebs" /></p>
<p><strong>Note that</strong> you can attach multiple EBS volumes to a single instance. <em>The volume and instance must be in the same Availability Zone.</em></p>
<p>Reference</p>
<p><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Storage.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Storage.html</a></p>
<h4 id="snapshots-encryption"><a class="header" href="#snapshots-encryption">Snapshots Encryption</a></h4>
<p><img src="aws/../img/aws-ebs-snapshots.svg" alt="ebs-snapshosts" /></p>
<h2 id="iops-and-disk-throughput"><a class="header" href="#iops-and-disk-throughput">IOPS and Disk Throughput</a></h2>
<p>AWS measures storage performance in <em>input/output operations per second</em>.</p>
<p>MySQL and MariaDB have a page size of 16 KB. Hence, writing 16 KB of data to disk would constitute one I/O operation.
Suppose your MySQL database needs to read 102,400 KB (100 MB) of data every second. Then the database needs to be able to read <code>102400 / 16 = 6400</code> 16 KB data, which requires your storage to be able to sustain 6400 IOPS.</p>
<p>Next consideration is the disk throughput - the data transfer rate.</p>
<p>Suppose you are running a MySQL, to sustain 2,000 Mbsp of disk throughput you need to divide the bandwidth by the page size, as follows:</p>
<pre><code class="language-ruby">16 KB * 8 = 128 Kb
128 Kb = 0.128 Mb
2,000 Mbsp / 0.128 Mb = 15,625 IOPS
</code></pre>
<h3 id="general-ssd-storage-gp2"><a class="header" href="#general-ssd-storage-gp2">General SSD Storage (gp2)</a></h3>
<p>For each gigabyte of data that you allocate to a volume, RDS allocates that volume a baseline performance of three IOPS, up to a total of 16,000 IOPS per volume. </p>
<p>A 20 GB volume would get 60 IOPS, whereas a 100 GB volume would get 300 IOPS. A 5,334 GB volume would get 16,000 IOPS. This means that the larger your volume, the better performance you’ll get. Thus the ratio of storage in gigabytes to IOPS is <code>3:1</code>.</p>
<p>To achieve 2,000 Mbps disk throughput to a volume, it would need 16,000 IOPS allo- cated. Note that you must specify provisioned IOPS in increments of 1,000. Also, as stated earlier, this is the maximum number of IOPS possible with gp2. To achieve this many IOPS, your volume would have to be 5,334 GB or about 5.33 TB.</p>
<h3 id="provisioned-iops-ssd-io1"><a class="header" href="#provisioned-iops-ssd-io1">Provisioned IOPS SSD (io1)</a></h3>
<p>The ratio of storage in gigabytes to IOPS must be at least <code>50:1</code>. For example, if you want 32,000 IOPS, you must provision at least 640 GB of storage.</p>
<p>You can provision up to 64,000 IOPS per io1 volume.</p>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><h1 id="ec2"><a class="header" href="#ec2">EC2</a></h1>
<h1 id="tenancy-placement"><a class="header" href="#tenancy-placement">Tenancy Placement</a></h1>
<p>Tenancy defines how EC2 instances are distributed across physical hardware and affects pricing. There are three tenancy options available: </p>
<ul>
<li>
<p>Shared (<code>default</code>) — Multiple AWS accounts may share the same physical hardware.</p>
</li>
<li>
<p>Dedicated Instance (<code>dedicated</code>) - Ensures all EC2 instances that are launched in a VPC run on hardware that's dedicated to a single customer <em>( Dedicated Instances may share hardware with other instances from the same AWS account that are not Dedicated Instances)</em>. </p>
</li>
<li>
<p>Dedicated Host (<code>host</code>) — Your instance runs on a physical server with EC2 instance capacity fully dedicated to your use, an isolated server with configurations that you can control <em>(To use a tenancy value of host, you must use a launch template)</em>. </p>
</li>
</ul>
<h3 id="vpc-tenancy"><a class="header" href="#vpc-tenancy">VPC tenancy</a></h3>
<p>When you create a launch configuration, the default value for the instance placement tenancy is null and the instance tenancy is controlled by the tenancy attribute of the VPC. </p>
<table><thead><tr><th align="left">Launch configuration tenancy</th><th align="left">VPC tenancy=default</th><th align="left">VPC tenancy=dedicated</th></tr></thead><tbody>
<tr><td align="left">not specified</td><td align="left">shared-tenancy instances</td><td align="left">Dedicated Instances</td></tr>
<tr><td align="left"><code>default</code></td><td align="left">shared-tenancy instances</td><td align="left">Dedicated Instances</td></tr>
<tr><td align="left"><code>dedicated</code></td><td align="left">Dedicated instances</td><td align="left">Dedicated Instances</td></tr>
</tbody></table>
<p>Note that: </p>
<ul>
<li>Some <code>AWS services or their features</code> won't work with a VPC with the instance tenancy set to dedicated.</li>
<li>Some <code>instance types</code> cannot be launched into a VPC with the instance tenancy set to dedicated. </li>
</ul>
<h1 id="placement-group"><a class="header" href="#placement-group">Placement Group</a></h1>
<ul>
<li>
<p><code>Cluster</code> groups launch each associated instance into a <strong>single availability zone</strong> within close physical proximity to each other. This provides <strong>low-latency</strong> network interconnectivity and can be useful for <strong>high-performance</strong> computing (HPC) applications, for instance.</p>
</li>
<li>
<p><code>Spread</code> groups separate instances physically across distinct hardware racks and even availability zones to reduce the risk of failure-related data or service loss. Such a setup can be valuable when you’re running hosts that <strong>can’t tolerate multiple concurrent failures</strong>.</p>
</li>
<li>
<p><code>Partition</code> groups is in the middle, that places a small group of instances across distinct underlying hardware to reduce correlated failures. Partition placement groups can be used to deploy large distributed and <strong>replicated</strong> workloads, such as HDFS, HBase, and Cassandra, across distinct racks.</p>
</li>
</ul>
<h1 id="cost-efficiency"><a class="header" href="#cost-efficiency">Cost Efficiency</a></h1>
<p>Reserved instance?</p>
<ul>
<li><a href="https://aws.amazon.com/blogs/aws/new-savings-plans-for-aws-compute-services/">https://aws.amazon.com/blogs/aws/new-savings-plans-for-aws-compute-services/</a></li>
</ul>
<h3 id="read-more"><a class="header" href="#read-more">Read More:    	</a></h3>
<ul>
<li><a href="https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-dedicated-instances.html">Configuring instance tenancy with a launch configuration</a></li>
<li><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html">Placement Group</a></li>
</ul>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><h1 id="iam"><a class="header" href="#iam">IAM</a></h1>
<p>An AWS account has two types of users, root user and IAM user. And they are belong to the same AWS account. When we login onto AWS management console, both root user and IAM user share the same account ID.</p>
<h2 id="iam-user-policies"><a class="header" href="#iam-user-policies">IAM user policies</a></h2>
<p>IAM user policies are identity-based policies that control an IAM user's access to account resources, such as a S3 bucket.</p>
<ul>
<li><code>Action</code> element refers to the kind of action requested (list, create, etc.);</li>
<li><code>Resource</code> element refers to the particular AWS <strong>account resource</strong> that’s the target of the policy;</li>
<li><code>Effect</code> element refers to the way IAM should react to a request.</li>
</ul>
<p>The user policy example below allows an IAM user to upload and read objects in <code>awsexamplebucket</code> S3 bucket.</p>
<pre><code class="language-json">{
  &quot;Version&quot;:&quot;2012-10-17&quot;,
  &quot;Statement&quot;: [
    {
      &quot;Effect&quot;:&quot;Allow&quot;,
      &quot;Action&quot;:[
        &quot;s3:PutObject&quot;,
        &quot;s3:GetObject&quot;
      ],
      &quot;Resource&quot;:&quot;arn:aws:s3:::awsexamplebucket/*&quot;
    }
  ]
}
</code></pre>
<p>Read more at 
<a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/user-policies.html">https://docs.aws.amazon.com/AmazonS3/latest/userguide/user-policies.htm</a></p>
<h2 id="bucket-policies"><a class="header" href="#bucket-policies">Bucket policies</a></h2>
<p>Bucket policies are resource-based policies.</p>
<p>If an AWS account that <strong>owns a bucket</strong> wants to grant permission to <strong>users in its account</strong>, it can use either a bucket policy or a user policy.</p>
<p>However, if we want manage cross-account access to a bucket, then we have to use bucket policies.</p>
<p>For instance, we can use the bucket policy below to grant permissions to other AWS accounts, <code>AccountB</code> to upload objects to the bucket <code>awsexamplebucket</code> that we own.</p>
<pre><code class="language-json">{
  &quot;Version&quot;:&quot;2012-10-17&quot;,
  &quot;Statement&quot;: [
    {
      &quot;Sid&quot;:&quot;AddCannedAcl&quot;,
      &quot;Effect&quot;:&quot;Allow&quot;,
      &quot;Principal&quot;: {&quot;AWS&quot;: &quot;arn:aws:iam::AccountB-ID:user/Dave&quot;},
      &quot;Action&quot;: [&quot;s3:PutObject&quot;,&quot;s3:PutObjectAcl&quot;],
      &quot;Resource&quot;: &quot;arn:aws:s3:::awsexamplebucket/*&quot;,
      &quot;Condition&quot;: {
        &quot;StringEquals&quot;: {
          &quot;s3:x-amz-acl&quot;: &quot;bucket-owner-full-control&quot;
         }
       }
     }
   ]
}
</code></pre>
<p>The <code>Condition</code> in the example makes sure that the owner of the bucket, <code>AccountA</code> has full control over the uploaded objects. Read more at <a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/amazon-s3-policy-keys.html">https://docs.aws.amazon.com/AmazonS3/latest/userguide/amazon-s3-policy-keys.html</a></p>
<p>After we add this bucket policy, user Dave must include the required ACL as part of the request:</p>
<pre><code class="language-bash">aws s3 cp example.jpg s3://awsexamplebucket --acl bucket-owner-full-control
</code></pre>
<p>Read more about <a href="https://aws.amazon.com/premiumsupport/knowledge-center/s3-require-object-ownership/">s3-require-object-ownership</a></p>
<p>This example is about cross-account permission. However, if Dave (who is getting the permission) belongs to the AWS account that owns the bucket, this conditional permission is not necessary. This is because the parent account to which Dave belongs owns objects that the user uploads. </p>
<p>Read more about bucket policies at 
<a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucket-policies.html">https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucket-policies.html</a></p>
<h2 id="object-acl-and-bucket-acl"><a class="header" href="#object-acl-and-bucket-acl">Object ACL and Bucket ACL</a></h2>
<p>Read more at <a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/access-policy-alternatives-guidelines.html">https://docs.aws.amazon.com/AmazonS3/latest/userguide/access-policy-alternatives-guidelines.html</a></p>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><h1 id="iam-role"><a class="header" href="#iam-role">IAM role</a></h1>
<p>IAM roles enables cross user or cross account temporary access for account resources.</p>
<p><img src="aws/../img/aws-iamrole.svg" alt="iamrole" /></p>
<p>An IAM role is not assigned to a user (by an admin). Rather, the IAM user assumes the role created by the admin.
Therefore, the admin needs to ensure that the user (trusted entity) has the permission to perform the <code>sts:AssumeRole</code> operation (action).</p>
<p>To provide such a permission, the admin needs to create an <code>IAM Policy</code> and attach it to the user or group.</p>
<pre><code class="language-json">{
    &quot;Version&quot;: &quot;2012-10-17&quot;,
    &quot;Statement&quot;: [
        {
            &quot;Sid&quot;: &quot;VisualEditor0&quot;,
            &quot;Effect&quot;: &quot;Allow&quot;,
            &quot;Action&quot;: &quot;sts:AssumeRole&quot;,
            &quot;Resource&quot;: &quot;arn:aws:iam::&lt;admin_account-id&gt;:role/*&quot; // any roles in this account
        }
    ]
}
</code></pre>
<p>With this IAM policy attached to the user, the user is now able to <strong>perform</strong> the <code>sts:AssumeRole</code> operationa. However, this does not mean that the user will get the role.</p>
<p>It is like </p>
<pre><code>You are now allowed to ask questions, but you may or may not get an answer&quot;.
</code></pre>
<p>Whether you will get the answer or not is determined by the <code>Trusted entity</code> which is covered below.</p>
<h3 id="create-the-role"><a class="header" href="#create-the-role">Create the role</a></h3>
<p>Next, we can create the IAM role.</p>
<p>An IAM Role consists of the following core elements:</p>
<ul>
<li>
<p><strong>Permission</strong>
specifies what account resources can be accessed and what actions can be taken, which is exactly what the IAM Policy does. For instance: adding <code>AmazonS3FullAccess</code> to the role permissions will allow the user who has successfully assumed this role to have full access to <code>S3</code>.</p>
</li>
<li>
<p><strong>Trusted Entity</strong>
specifies what entitiy can assume <strong>this role</strong> (Don't be confused with the IAM Policy <code>sts:AssumeRole</code> action above).</p>
</li>
</ul>
<pre><code class="language-json">{
  &quot;Version&quot;: &quot;2012-10-17&quot;,
  &quot;Statement&quot;: [
    {
      &quot;Effect&quot;: &quot;Allow&quot;,
      &quot;Principal&quot;: {
		&quot;AWS&quot;: &quot;arn:aws:iam::&lt;user_account-id&gt;:root&quot; // the user who performs the AssumeRole action
      },
      &quot;Action&quot;: &quot;sts:AssumeRole&quot;,
      &quot;Condition&quot;: {}
    }
  ]
}
</code></pre>
<h3 id="assume-the-role"><a class="header" href="#assume-the-role">Assume the role</a></h3>
<p>Before we assume the role, let's first verify that we don't have access to <code>S3</code>.</p>
<pre><code class="language-bash">aws s3 ls
</code></pre>
<p>Next, let's assume the role <code>s3fullaccess-user1</code> created above.</p>
<p>One way is to add a <code>profile</code> to <code>~/.aws/config</code>, as shown below.
For simplicity, we use the role name <code>s3fullaccess-user1</code> as the <code>[profile_name]</code>.</p>
<pre><code>[s3fullaccess-user1]
role_arn=arn:aws:iam::&lt;admin_account-id&gt;:role/s3fullaccess-user1
source_profile=account1
</code></pre>
<p>Now if we invoke the command with the <code>s3fullaccess-user1</code> profile, we will be able to list the buckets in <code>S3</code>.</p>
<pre><code class="language-bash">aws s3 ls --profile s3fullaccess-user1
</code></pre>
<p>Read more about the how to config <code>awscli</code> to use an IAM role <a href="https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-role.html">here</a>.</p>
<p>Anothe way to consume the IAM role is to use <code>awscli</code>:</p>
<pre><code class="language-bash">aws sts assume-role --role-arn &quot;arn:aws:iam::&lt;admin_account-id&gt;:role/s3fullaccess-user1&quot; --role-session-name AWSCLI-Session
</code></pre>
<p>A full example for assuming IAM role using <code>awscli</code> is <a href="https://aws.amazon.com/premiumsupport/knowledge-center/iam-assume-role-cli/">here</a></p>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><h1 id="vpc"><a class="header" href="#vpc">VPC</a></h1>
<p><img src="aws/../img/aws-vpc.svg" alt="vpc" /></p>
<h2 id="subnets"><a class="header" href="#subnets">Subnets</a></h2>
<p>VPC CIDR: 172.16.0.0/16</p>
<p>Given a subnet CIDR within the VPC, as follows:</p>
<p>172.16.17.30/20</p>
<p>10101100.00010000.<strong>0001</strong>0001.00011110</p>
<p>The next non-overlaping CIDR (subnet) is to keep the first 16 bits of VPC CIDR prefix and only modify the next 4 bits that are highlighted. And we get the next subnet CIDR within the VPC:</p>
<p>172.16.33.30/20</p>
<p>10101100.00010000.<strong>0010</strong>0001.00011110</p>
<p>To get an IP address within a subnet, modify the bits other than the 20 bits of subnet CIDR prefix:</p>
<p>172.16.32.31 </p>
<p>10101100.00010000.0010<strong>0000.00011111</strong></p>
<h2 id="vpc-endpoint"><a class="header" href="#vpc-endpoint">VPC endpoint</a></h2>
<p>A <a href="https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints.html">VPC endpoint</a> enables private connections <strong>between your VPC and supported AWS services</strong> and <a href="https://docs.aws.amazon.com/vpc/latest/privatelink/endpoint-service.html">VPC endpoint services</a> powered by AWS PrivateLink.</p>
<p>A VPC endpoint does not require an internet gateway, virtual private gateway, NAT device, VPN connection, or AWS Direct Connect connection. Instances in your VPC do not require public IP addresses to communicate with resources in the service. </p>
<p>One usecase of VPC endpoint is to <a href="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/vpc-endpoints.html">create VPC endpoints for Amazon ECS</a></p>
<h2 id="dhcp-option-set"><a class="header" href="#dhcp-option-set">DHCP option set</a></h2>
<h3 id="read-more-1"><a class="header" href="#read-more-1">Read More</a></h3>
<ul>
<li><a href="https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Subnets.html">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Subnets.html</a></li>
<li><a href="https://www.cisco.com/c/en/us/support/docs/ip/routing-information-protocol-rip/13788-3.html">https://www.cisco.com/c/en/us/support/docs/ip/routing-information-protocol-rip/13788-3.html</a></li>
</ul>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><h1 id="hybrid-cloud"><a class="header" href="#hybrid-cloud">Hybrid Cloud</a></h1>
<p>Virtual private gateway
VPC peering</p>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><h1 id="elastic-load-balancing"><a class="header" href="#elastic-load-balancing">Elastic Load Balancing</a></h1>
<h3 id="public-application-load-balancer-with-ec2-auto-scaling-group-in-private-subnet"><a class="header" href="#public-application-load-balancer-with-ec2-auto-scaling-group-in-private-subnet">Public Application Load Balancer with EC2 Auto Scaling Group in Private Subnet</a></h3>
<p><img src="aws/../img/aws-elb.svg" alt="elb" /></p>
<h3 id="read-more-2"><a class="header" href="#read-more-2">Read More</a></h3>
<ul>
<li><a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/how-elastic-load-balancing-works.html">Elastic Load Balancing</a></li>
<li><a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/application/introduction.html">Application Load Balancer</a></li>
<li><a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html">Network Load Balancer</a></li>
</ul>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><h1 id="auto-scaling"><a class="header" href="#auto-scaling">Auto Scaling</a></h1>
<p>Suppose, we launch 10 instances in an Auto Scaling Group and maitain the <code>AverageCPUUsage</code> between 40% and 60%.</p>
<pre><code>Metric value

-infinity          30%    40%          60%     70%             infinity
-----------------------------------------------------------------------
          -30%      | -10% | Unchanged  | +10%  |       +30%        
-----------------------------------------------------------------------
</code></pre>
<ul>
<li>If the metric value gets to 60, the desired capacity of the group increases by 1 instance, to 11 instances, based on the second step adjustment of the scale-out policy (add 10 percent of 10 instances).</li>
<li>If the metric value rises to 70 even after this increase in capacity, the desired capacity of the group increases by another 3 instances, to 14 instances. This is based on the third step adjustment of the scale-out policy (add 30 percent of 11 instances, 3.3 instances, rounded down to 3 instances). </li>
<li>If the metric value gets to 40, the desired capacity of the group decreases by 1 instance, to 13 instances, based on the second step adjustment of the scale-in policy (remove 10 percent of 14 instances, 1.4 instances, rounded down to 1 instance).</li>
<li>If the metric value falls to 30 even after this decrease in capacity, the desired capacity of the group decreases by another 3 instances, to 10 instances. This is based on the third step adjustment of the scale-in policy (remove 30 percent of 13 instances, 3.9 instances, rounded down to 3 instances). </li>
</ul>
<p>You can use a single step policy or create multiple (at least 4) simple policies to achieve the scaling above. However, the differences are: </p>
<ul>
<li>
<p>With step scaling policies, you can specify instance <code>warm-up</code> time - the number of seconds for a newly launched instance to warm up. Using the example in the Step Adjustments section, suppose that the metric gets to 60, and then it gets to 62 while the new instance is still warming up. The current capacity is still 10 instances, so 1 instance is added (10 percent of 10 instances). However, the desired capacity of the group is already 11 instances, so the scaling policy does not increase the desired capacity further. If the metric gets to 70 while the new instance is still warming up, we should add 3 instances (30 percent of 10 instances). However, the desired capacity of the group is already 11, so we add only 2 instances, for a new desired capacity of 13 instances. </p>
</li>
<li>
<p>With simple scaling policies, after a scaling activity is started, the policy must wait for the scaling activity or health check replacement to complete and the cooldown period to expire before responding to additional alarms. Cooldown periods help to prevent the initiation of additional scaling activities before the effects of previous activities are visible. In contrast, with step scaling the policy can continue to respond to additional alarms, even while a scaling activity or health check replacement is in progress. </p>
</li>
</ul>
<h3 id="read-more-3"><a class="header" href="#read-more-3">Read More</a></h3>
<ul>
<li><a href="https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-simple-step.html">Step and simple scaling poclies</a></li>
</ul>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><h1 id="route-53"><a class="header" href="#route-53">Route 53</a></h1>
<p>You can use Route 53 <a href="https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-failover">failover routing</a> policy to route traffic to the secondary cluster deployed in another region in case outage of the primary cluster. Note that <code>Simple Routing Policy</code> does not support health-checks.</p>
<p><img src="aws/../img/aws-53-failover.svg" alt="failover" /></p>
<p>As Elastic Load Blancer does not support RDS instances, you can, however, use Route 53 <a href="https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routing-policy-weighted">weighted routing</a> policy to distribute traffic across the RDS Read Replicas. In case of a Read Replica health-check failure, Route 53 weighted record will exlcude those adresses in its reponse to a DNS query.</p>
<p><img src="aws/../img/aws-53-weighted.svg" alt="weighted" /></p>
<p>Reference</p>
<ul>
<li><a href="https://aws.amazon.com/blogs/database/scaling-your-amazon-rds-instance-vertically-and-horizontally/">Scale RDS instances</a></li>
<li><a href="https://aws.amazon.com/premiumsupport/knowledge-center/requests-rds-read-replicas/">Distribute requests to RDS read replicas</a></li>
</ul>
<p>Note that <a href="https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.Endpoints.html">a reader endpoint for an Aurora DB cluster provides load-balancing support for read-only connections to the DB cluster.</a></p>
<p>Read more</p>
<ul>
<li><a href="https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-to-aws-resources.html">Routing internet traffic to AWS resources using Route 53</a></li>
<li><a href="https://docs.aws.amazon.com/whitepapers/latest/real-time-communication-on-aws/cross-region-dns-based-load-balancing-and-failover.html">Cross-Region DNS-Based Load Balancing and Failover</a></li>
</ul>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><h1 id="global-accelerator"><a class="header" href="#global-accelerator">Global Accelerator</a></h1>
<p>AWS Global Accelerator is a networking service that helps you improve the <strong>availability</strong> and performance of the applications that you offer to your global users.</p>
<h2 id="vs-elb"><a class="header" href="#vs-elb">VS. ELB</a></h2>
<p>ELB provides load balancing within one Region, AWS Global Accelerator provides traffic management across multiple Regions.
A regional ELB load balancer is an ideal target for AWS Global Accelerator. AWS Global Accelerator complements ELB by extending these capabilities beyond a single AWS Region, allowing you to provision a global interface for your applications in any number of Regions. </p>
<h2 id="vs-cloudfront"><a class="header" href="#vs-cloudfront">VS. CloudFront</a></h2>
<p>AWS Global Accelerator and Amazon CloudFront are separate services that use the AWS global network and its edge locations around the world.</p>
<p>Both services integrate with AWS Shield for DDoS protection.</p>
<ul>
<li>CloudFront improves performance for both cacheable content (such as images and videos) and dynamic content (such as API acceleration and dynamic site delivery).</li>
<li>Global Accelerator is a good fit for non-HTTP use cases, such as gaming (UDP), IoT (MQTT), or Voice over IP, as well as for HTTP use cases that specifically require static IP addresses or deterministic, fast regional failover.</li>
</ul>
<h2 id="vs-route-53"><a class="header" href="#vs-route-53">VS. Route 53</a></h2>
<p>In short: AWS Global Accelerator doesn't have problems with DNS record caching TTL when failing over to another region.</p>
<ul>
<li>First, some client devices and internet resolvers cache DNS answers for long periods of time. So when you make a configuration update, or there’s an application failure or change in your routing preference, you don’t know how long it will take before all of your users receive updated IP addresses. With AWS Global Accelerator, you don’t have to rely on the IP address caching settings of client devices. Change propagation takes a matter of seconds, which reduces your application downtime.</li>
<li>Second, with Global Accelerator, you get static IP addresses that provide a fixed entry point to your applications. This lets you easily move your endpoints between Availability Zones or between AWS Regions, without having to update the DNS configuration or client-facing applications.</li>
</ul>
<p>You can create a Route 53 record to point to AWS Global Accelerator alias.</p>
<h3 id="read-more-4"><a class="header" href="#read-more-4">Read More</a></h3>
<ul>
<li><a href="https://aws.amazon.com/global-accelerator/faqs/">AWS Global Accelerator FAQs</a></li>
<li><a href="https://docs.aws.amazon.com/global-accelerator/latest/dg/dns-addressing-custom-domains.dns-addressing.html">DNS addressing in Global Accelerator</a></li>
</ul>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><h1 id="cloud-front"><a class="header" href="#cloud-front">Cloud Front</a></h1>
<h2 id="cloudfront-caching"><a class="header" href="#cloudfront-caching">CloudFront Caching</a></h2>
<ul>
<li>Caching based on Query String</li>
<li>Caching based on cookies</li>
<li>Caching based on request headers</li>
</ul>
<h2 id="cloudfront-private-content"><a class="header" href="#cloudfront-private-content">CloudFront private content</a></h2>
<ul>
<li>Signed URL</li>
<li>Signed Cookies</li>
<li>Origin Access Identify for restricting access to S3 buckets.</li>
<li>Custom Headers for restricting access to ELB</li>
</ul>
<h4 id="read-more-5"><a class="header" href="#read-more-5">Read More</a></h4>
<ul>
<li><a href="https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/ConfiguringCaching.html">CloudFront Caching</a></li>
<li><a href="https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-overview.html">CloudFront Private Content</a></li>
</ul>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><h1 id="storage"><a class="header" href="#storage">Storage</a></h1>
<p>Aurora vertical scaling?</p>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><h1 id="s3-and-glacier"><a class="header" href="#s3-and-glacier">S3 and Glacier</a></h1>
<ul>
<li>Glacier supports archives as large as 40 TB, whereas S3 buckets have no size limit.</li>
<li>Glacier <a href="https://docs.aws.amazon.com/amazonglacier/latest/dev/amazon-glacier-data-model.html">archives</a> are encrypted by default, whereas encryption on S3 is an option you need to select.</li>
<li>Unlike S3’s “human-readable” key names, Glacier archives are given machine-generated IDs.</li>
</ul>
<p>The biggest difference is the time it takes to retrieve your data. Retrieving the objects from an existing Glacier archive can take a number of hours. You can specify one of the following when initiating a job to retrieve an archive based on your access time and cost requirements:</p>
<ul>
<li><strong>Expedited</strong> - data accessed using Expedited retrievals are typically made available within 1–5 minutes</li>
<li><strong>Standard</strong> - Standard retrievals typically complete within 3–5 hours.</li>
<li><strong>Bulk</strong> - Bulk retrievals typically complete within 5–12 hours.</li>
</ul>
<p>Reference </p>
<ul>
<li><a href="https://docs.aws.amazon.com/amazonglacier/latest/dev/downloading-an-archive-two-steps.html">Downloading an Archive</a></li>
</ul>
<h3 id="s3-range-get"><a class="header" href="#s3-range-get">S3 Range GET?</a></h3>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><h1 id="replication--backups"><a class="header" href="#replication--backups">Replication &amp; Backups</a></h1>
<h3 id="s3"><a class="header" href="#s3">S3</a></h3>
<p>All S3 storage classes except <code>One Zone-Infrequent Access</code> distribute objects across multiple
availability zones.</p>
<p>You can also enable cross-region replication between a source bucket in one region and destination bucket in another.
Note that cross-region replication requires versioning to be enabled on both buckets. Note that cross-region replication does not apply to existing objects. Also, if the source bucket get deleted,the target bucket is NOT deleted. For replicating object deletion, you can <a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/delete-marker-replication.html">enable delete marker replication</a></p>
<p>In addtion to <a href="https://aws.amazon.com/about-aws/whats-new/2018/11/s3-intelligent-tiering/">S3 Intelligent Tier Storage</a> that moves objects that have not been accessed for 30 consecutive days to the infrequent access tier. You can also manually configure <a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-mgmt.html">lifecycle rules</a> for a bucket that will automatically transition an object’s storage class after a set number of days. </p>
<p>Read more about <a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/replication.html#crr-scenario">Replicating objects</a>.</p>
<h3 id="ebs-volume"><a class="header" href="#ebs-volume">EBS Volume</a></h3>
<p>EBS automatically replicates volumes across multiple availability zones in a region.</p>
<p>You can use <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/snapshot-lifecycle.html">Amazon Data Lifecycle Manager</a> to automatically create a snapshot for you at regular intervals. To use the Amazon Data Lifecycle Manager, you create a Snapshot Lifecycle Policy and specify an interval of up to 24 hours, as well as a snapshot creation time. You also must specify the number of automatic snapshots to retain, up to 1,000
snapshots. You can also enable <code>cross-region</code> copy and cross-account sharing.</p>
<h3 id="efs"><a class="header" href="#efs">EFS</a></h3>
<p>EFS filesystems are stored across multiple zones in a region.</p>
<p>To protect against data loss and corruption, you can back up individual files to an S3 bucket or another EFS filesystem in the same region. 
You can also use the <a href="https://docs.aws.amazon.com/efs/latest/ug/awsbackup.html">AWS Backup Service</a> to schedule incremental backups of your EFS filesystem</p>
<h3 id="dynamodb"><a class="header" href="#dynamodb">DynamoDB</a></h3>
<p>DynamoDB stores tables across multiple availability zones.
To replicate DynamoDB table to different region, you can use <a href="https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GlobalTables.html">DynamoDB global tables</a>. </p>
<p>You can also configure point-in-time recovery to automatically take backups of your DynamoDB tables.
Point-in-time recovery lets you restore your DynamoDB tables to any point in time from 35 days until 5 minutes before the current time.</p>
<h3 id="rds"><a class="header" href="#rds">RDS</a></h3>
<h4 id="multi-az-deployments"><a class="header" href="#multi-az-deployments">Multi-AZ Deployments</a></h4>
<p>Multi-AZ deployment provides a <em>standby database instance</em> in a different availibity zone that the <em>primary database</em> instance resides.</p>
<p>RDS synchronously replicates data from the primary to the standby instance. And if the primray instance experiences an outage, it will fail over to the standby instance. </p>
<p><strong>Note that</strong> in the multi-AZ deployment, all instances resides in the same region. And the standby instance is not a read replica and cannot serve read traffic.</p>
<h4 id="read-replica"><a class="header" href="#read-replica">Read Replica</a></h4>
<p>With Amazon RDS, you can create a read replica of the primary database instance in a different AZ even different region.
However, creating a cross-region read replica isn't supported for SQL Server on Amazon RDS.</p>
<p>When creating a read replica, you must enable automatic backups on the source DB instance by setting the backup retention period to
a value other than 0.</p>
<p>You can have up to 5 replicas and each read replica will have its own DNS endpoint.</p>
<p>Replicas can be promoted to their own standalone database, but this breaks the replication. 
Moreover, no automatic failover, if primary database fails you must manually update urls to point at the newly promoted database.</p>
<h5 id="compare"><a class="header" href="#compare">Compare</a></h5>
<table><thead><tr><th align="left">Mutil-AZ Deployments</th><th align="left">Read Replicas</th></tr></thead><tbody>
<tr><td align="left">Synchronous replication - highly durable</td><td align="left">Asynchronous replication - highly scalable</td></tr>
<tr><td align="left">Only database engine on primary instance is active</td><td align="left">All read replicas are accessible and can be used for read scaling</td></tr>
<tr><td align="left">Automated backups are taken from standby instance</td><td align="left">No backups configured by default</td></tr>
<tr><td align="left">Always span at least 2 Availability Zones within a single region</td><td align="left">Can be within Availability Zone, Cross-AZ, or Cross-Region</td></tr>
<tr><td align="left">Database engine version upgrades happen on primary instance</td><td align="left">Database engine version upgrade is independent from source instance</td></tr>
<tr><td align="left">Automatic failover to standby when a problem is detected</td><td align="left">can be manually promoted to a standalone database instance</td></tr>
</tbody></table>
<h4 id="point-in-time-recovery"><a class="header" href="#point-in-time-recovery">Point-in-time recovery</a></h4>
<p>Enabling automatic backups enables point-in-time recovery, which archives database change logs to S3 every 5 minutes.</p>
<p>RDS keeps automated snapshots for a limited period of time and then deletes them. You can choose a retention period between one day and 35 days. 
The default is seven days. To disable automated snapshots, set the retention period to 0. </p>
<p>Note that disabling automated snapshots immediately deletes all existing automated snapshots and disables point-in-time recovery. Also, if you change the retention period from 0 to any other value, it will trigger an immediate snapshot.</p>
<p>Reference</p>
<ul>
<li><a href="https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html">Working with read replicas</a></li>
<li><a href="https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.XRgn.html">Cross-region read replicas</a></li>
<li><a href="https://aws.amazon.com/rds/features/read-replicas/">Read replicas, Mutil-AZ deployments, and multi-region deployments</a></li>
</ul>
<h3 id="aurora"><a class="header" href="#aurora">Aurora</a></h3>
<ul>
<li><a href="https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.html">Aurora DB cluster</a></li>
<li><a href="https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.Endpoints.html">Aurora Connection</a></li>
<li><a href="https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html">Aurora Replication</a></li>
</ul>
<h3 id="redshift"><a class="header" href="#redshift">Redshift</a></h3>
<p>Snapshots are point-in-time backups of a cluster. There are two types of snapshots: automated and manual. Amazon Redshift stores these snapshots internally in Amazon S3 by using an encrypted Secure Sockets Layer (SSL) connection. </p>
<p>When automated snapshots are enabled for a cluster, Amazon Redshift periodically takes snapshots of that cluster. By default Amazon Redshift takes a snapshot about every eight hours or following every 5 GB per node of data changes, or whichever comes first. </p>
<p>You can configure Amazon Redshift to automatically copy snapshots (automated or manual) for a cluster to another AWS Region. When a snapshot is created in the cluster's primary AWS Region, it's copied to a secondary AWS Region.</p>
<p>Reference</p>
<ul>
<li><a href="https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-snapshots.html#working-with-snapshots-overview">https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-snapshots.html#working-with-snapshots-overview</a></li>
</ul>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><h1 id="elasticache"><a class="header" href="#elasticache">ElastiCache</a></h1>
<p>A <em>node</em> is the smallest buidling block of an ElastiCache deployment. Each node has its own DNS name and port.</p>
<p>Both Redis and Memcached cluster need to be run in a VPC, a collection of subnets within that VPC.</p>
<h2 id="redis"><a class="header" href="#redis">Redis</a></h2>
<p>A Redis <em>shard</em> is a grouping of one to six related nodes. Redis clusters can have up to 500 shards, with data partitioned across the shards.</p>
<p>Redis Repliation happens in a shard where one of the nodes is the read/write primary node. All the other nodes are read-only replica nodes. Locating read replicas in multiple Availibility Zones improves fault tolerance.</p>
<p>The primary endpoint of a Redis cluster is a DNS name that always resolves to the primary node in the cluster. The primary endpoint is <strong>immune</strong> to changes to the cluster, such as promoting a read replica to the primary role. Note that Read Replica failover or promotion is happened automatically which is different to RDS (non-Aurora) database.</p>
<p>The reader endpoint of a Redis cluster will evenly split incoming connections to the endpoint between all read replicas in a ElastiCache for Redis cluster.</p>
<p>In addition, you can use <a href="https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/Redis-Global-Datastore.html">global datastores</a> to create cross-region read replica clusters for ElastiCache for Redis to enable low-latency reads and disaster recovery across AWS Regions.</p>
<h2 id="memchached"><a class="header" href="#memchached">Memchached</a></h2>
<p>A Memcached cluster is a logical grouping of one or more ElastiCache nodes. Data is partitioned across the nodes in the Memcached cluster. A Memcached cluster can have up to 300 nodes. And nodes can be run across multiple Availibility Zones.</p>
<p>A Memcached cluster does not have the concept of <em>sharding</em> as Redis. Memcached cluster relies on the client <a href="https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/AutoDiscovery.html">Auto-Discovery</a> to automatically identify all of the nodes in a cached cluster, and to initiate and maitain connetions to all of these nodes.</p>
<h3 id="comparing-between-redis-and-memecached"><a class="header" href="#comparing-between-redis-and-memecached">Comparing between Redis and Memecached</a></h3>
<p><a href="https://aws.amazon.com/elasticache/redis-vs-memcached/">https://aws.amazon.com/elasticache/redis-vs-memcached/</a></p>
<style>
table {
  margin: 0!important;
}
</style>
<table><thead><tr><th align="left"></th><th align="left">Memcached</th><th align="left">Redis</th></tr></thead><tbody>
<tr><td align="left">Data Partitioning</td><td align="left">Yes</td><td align="left">Yes</td></tr>
<tr><td align="left">Advanced data structures</td><td align="left">-</td><td align="left">Yes</td></tr>
<tr><td align="left">Multithreaded architecture</td><td align="left">Yes</td><td align="left">-</td></tr>
<tr><td align="left">Snapshots/Backups</td><td align="left">-</td><td align="left">Yes</td></tr>
<tr><td align="left">Replication</td><td align="left">-</td><td align="left">Yes</td></tr>
</tbody></table>
<h3 id="read-more-6"><a class="header" href="#read-more-6">Read More</a></h3>
<ul>
<li><a href="https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/WhatIs.Components.html">Elasticache Redis</a></li>
<li><a href="https://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/WhatIs.Components.html">Elasticache Memcached</a></li>
</ul>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><h1 id="security"><a class="header" href="#security">Security</a></h1>
<p><img src="aws/../img/aws-security.svg" alt="security" /></p>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><h1 id="data-encryption"><a class="header" href="#data-encryption">Data Encryption</a></h1>
<h2 id="data-at-rest"><a class="header" href="#data-at-rest">Data at Rest</a></h2>
<p>Data stored in Glacier and Storage Gateway are encrypted by default.</p>
<h2 id="data-in-trasit"><a class="header" href="#data-in-trasit">Data in Trasit</a></h2>
<ul>
<li>All traffic between AZs is encrypted</li>
<li>All data that <code>Storage Gateway</code> transfers to AWS is encrypted in transit and at rest in AWS.</li>
<li>Amazon Certificate Manager (ACM) to generate a TLS certifi- cate and then install it on an application load balancer or a CloudFront distribution</li>
</ul>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><h1 id="linux"><a class="header" href="#linux">Linux</a></h1>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><h2 id="volume-1"><a class="header" href="#volume-1">Volume</a></h2>
<p>Files are stored on random-access storage devices, including hard disks, solid-state disks.</p>
<p>Files can only be stored on a storage device with a file system created. Any entity containing a file system is generally known as a volume.</p>
<p>The volume may be partition or a whole device. Each volume that contains a file system must also contain information about the files in the system. This information is kept in entries in a <strong>device directory</strong> that records informtation -- such as name, location, size, and type -- for all files on that volume.</p>
<p><img src="linux/../img/os-volume.svg" alt="volume" /></p>
<h3 id="volume-mounting"><a class="header" href="#volume-mounting">Volume Mounting</a></h3>
<p>Just as a file must be opened before it is used, a file system (volume) must be mounted beforeit can be availble on the operating system.</p>
<p>The mount procedure is straightforward. The operating system is given the name of the device and the <strong>mount point</strong> -- where the file system is to be attached.</p>
<h4 id="example-aws-ebs-volume"><a class="header" href="#example-aws-ebs-volume">Example (AWS EBS volume)</a></h4>
<p>When we launch an AWS EC2 instance, we can add additional storage, by attaching additional <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volumes.html">EBS volumes</a>.</p>
<table><thead><tr><th>Volume Type</th><th>Device</th><th>Snapshot</th><th>Size(GiB)</th><th>Volume Type</th></tr></thead><tbody>
<tr><td>Root</td><td>/dev/xvda</td><td>snap-0ee8a4a337cf9d029</td><td>128</td><td>SSD</td></tr>
<tr><td>EBS</td><td>/dev/xvdb</td><td></td><td>8</td><td>SSD</td></tr>
</tbody></table>
<p>The EBS storage attached to the EC2 instance is not ready to be used since it doesn't contain a file system. We therefore need to create a file system on this storage device and mount it onto the EC2 instance OS.</p>
<p>We could <em>ssh</em> into the launched EC2 instance and issue the commands below, in order to store files into this EBS volume.</p>
<pre><code class="language-bash">$ lsblk
NAME     MAJ:MIN RM  SIZE RO  TYPE MOUNTPOINT
xvda     202:0    0  128G  0  disk
|_xvda1  202:1    0  128G  0  part /
xvdb     202:16   0    8G  0  disk
$ sudo su
$ file -s /dev/xvdb
/dev/xcdb: data
$ mkfs -t xfs /dev/xvdb
meta-data=/dev/xvdb   isize=512 agcount=4 agsize=524288 blks
.....
$ file -s /dev/xvdb
/dev/xvdb: SGI XFS filesystem data (blks 4096, inosz 512, v2 dirs)
$ makedir /data
$ mount /dev/xvdb /data
$ cd /data
$ touch test.txt
</code></pre>
<h4 id="example-install-void-linux"><a class="header" href="#example-install-void-linux">Example (Install Void Linux)</a></h4>
<p>When installing <a href="https://docs.voidlinux.org/installation/live-images/guide.html">Void Linux</a>, the essential parts are to partition the computer disk and create file systems. </p>
<p>Suppose the device name for the computer disk is <code>/dev/sda</code> and we need to partition the disk and create the file system for each partition as:</p>
<style>
table {
  margin: 0!important;
}
</style>
<table><thead><tr><th align="left">Device</th><th align="left">Size</th><th align="left">Type</th><th align="left">Mount point</th><th align="left">File system type</th></tr></thead><tbody>
<tr><td align="left">/dev/sda1</td><td align="left">100M</td><td align="left">BIOS boot</td><td align="left">/boot/efi</td><td align="left">vfat  FAT32</td></tr>
<tr><td align="left">/dev/sda2</td><td align="left">298G</td><td align="left">Linux filesystem</td><td align="left">/</td><td align="left">btrfs Oracle's Btrfs</td></tr>
</tbody></table>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><h1 id="install-void-linux"><a class="header" href="#install-void-linux">Install Void Linux</a></h1>
<h2 id="download--flash"><a class="header" href="#download--flash">Download &amp; Flash</a></h2>
<p>First download a <a href="https://docs.voidlinux.org/installation/index.html#downloading-installation-media">live image</a>. Download <code>void-live-x86_64-20210218-gnome.iso</code> if you want to use the <a href="https://www.gnome.org/">GNOME</a> desktop.</p>
<p>Next, flash the downloaded image to a USB drive. You can use <a href="https://www.balena.io/etcher/">etcher</a>.</p>
<h2 id="bios-settings"><a class="header" href="#bios-settings">Bios Settings</a></h2>
<p>I am installing void linux on a Lenove Thinkpad. Press <code>F12</code> to enter Bios setup.</p>
<p>In order to boot from a USB drive, we need to disable <code>Secure Boot</code> in the <strong>Security</strong> settings.</p>
<p>Next, in the Bios <strong>Startup</strong> settings, make sure we have <code>UEFI/Legacy Boot</code> set to <code>UEFI only</code></p>
<h2 id="install"><a class="header" href="#install">Install</a></h2>
<p>Before starting installation, make sure we are using UEFI booting by checking:</p>
<pre><code class="language-bash">ls /sys/firware/efi
# /sys/firware/efi exists means system uses UEFI
</code></pre>
<p>Type <code>sudo void-installer</code> in terminal, we will get into void linux installation wizard.</p>
<p><em>The <code>keymap</code> for swedish keyboard is <code>se-latin1</code>.</em></p>
<h3 id="bootloader"><a class="header" href="#bootloader">Bootloader</a></h3>
<p>Select the disk, for example <code>/dev/sda</code> to install the boot loader and choose graphical terminal for GRUB menu.</p>
<h3 id="partition"><a class="header" href="#partition">Partition</a></h3>
<p>For EFI systems GPT is mandatory and a FAT32 partition with at least 100MB must be created with the TOGGLE <code>boot</code>, this will be used as EFI System Partition. This partition must be mounted as <code>/boot/efi</code>.</p>
<p>At least 1 partition is required for the root file system <code>/</code>.</p>
<p>Therefore, we need at least 2 partitions for our computer disk (device name might be <code>/dev/sda</code>):</p>
<style>
table {
  margin: 0!important;
}
</style>
<table><thead><tr><th align="left">Device</th><th align="left">Size</th><th align="left">Type</th></tr></thead><tbody>
<tr><td align="left">/dev/sda1</td><td align="left">100M</td><td align="left">BIOS boot</td></tr>
<tr><td align="left">/dev/sda2</td><td align="left">298G</td><td align="left">Linux filesystem</td></tr>
</tbody></table>
<h3 id="file-system"><a class="header" href="#file-system">File system</a></h3>
<p>We need to create and mount the file systems for each of the 2 partitions:</p>
<table><thead><tr><th align="left">Device</th><th align="left">Mount point</th><th align="left">File system type</th></tr></thead><tbody>
<tr><td align="left">/dev/sda1</td><td align="left">/boot/efi</td><td align="left">vfat  FAT32</td></tr>
<tr><td align="left">/dev/sda2</td><td align="left">/</td><td align="left">btrfs Oracle's Btrfs</td></tr>
</tbody></table>
<h2 id="post-installation"><a class="header" href="#post-installation">Post Installation</a></h2>
<p>Now we have void linux installed. We need to <a href="https://docs.voidlinux.org/xbps/index.html#updating">perform a system update</a> for the first time:</p>
<pre><code class="language-bash">sudo xbps-install -u xbps
sudo xbps-install -Su
</code></pre>
<br>
Reference
<p><a href="https://docs.voidlinux.org/installation/live-images/guide.html">https://docs.voidlinux.org/installation/live-images/guide.html</a></p>
<div id="chapter_begin" style="break-before: page; page-break-before: always;"></div><h1 id="netcat"><a class="header" href="#netcat">netcat</a></h1>
<p>To scan a TCP port using <code>netcat</code>, use: </p>
<pre><code class="language-bash">nc -zv 13.49.227.25 80
</code></pre>
<ul>
<li>The <code>-z</code> option is for scanning without sending any data.</li>
<li>The <code>-v</code> option is to print verbose output.</li>
</ul>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        

                        

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                

                
            </nav>

        </div>

        

        

        

        
        <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        

        

        
        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        

        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        

        
        
        <script type="text/javascript">
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>
        
        

    </body>
</html>
